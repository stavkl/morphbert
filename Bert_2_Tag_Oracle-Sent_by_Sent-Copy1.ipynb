{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(3)\n",
    "np.random.seed(3)\n",
    "torch.cuda.manual_seed_all(3)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading .conll files into dataframes\n",
    "Of train, dev and test corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ZeroIx</th>\n",
       "      <th>Index</th>\n",
       "      <th>FORM</th>\n",
       "      <th>LEMMA</th>\n",
       "      <th>POS</th>\n",
       "      <th>PPOS</th>\n",
       "      <th>FEAT</th>\n",
       "      <th>CorrespondingTokenIx</th>\n",
       "      <th>sent_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11271</th>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>גופתו</td>\n",
       "      <td>גופה</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN_S_PP</td>\n",
       "      <td>gen=F|num=S|suf_gen=M|suf_num=S|suf_per=3</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11272</th>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>של</td>\n",
       "      <td>של</td>\n",
       "      <td>POS</td>\n",
       "      <td>POS</td>\n",
       "      <td>_</td>\n",
       "      <td>14</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11273</th>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>מאיר</td>\n",
       "      <td>מאיר</td>\n",
       "      <td>NNP</td>\n",
       "      <td>NNP</td>\n",
       "      <td>_</td>\n",
       "      <td>15</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11274</th>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>כהנא</td>\n",
       "      <td>כהנא</td>\n",
       "      <td>NNP</td>\n",
       "      <td>NNP</td>\n",
       "      <td>_</td>\n",
       "      <td>16</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11275</th>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>אל</td>\n",
       "      <td>אל</td>\n",
       "      <td>IN</td>\n",
       "      <td>IN</td>\n",
       "      <td>_</td>\n",
       "      <td>17</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11276</th>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>ה</td>\n",
       "      <td>ה</td>\n",
       "      <td>DEF</td>\n",
       "      <td>DEF</td>\n",
       "      <td>_</td>\n",
       "      <td>18</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11277</th>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>קבר</td>\n",
       "      <td>קבר</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>gen=M|num=S</td>\n",
       "      <td>18</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11278</th>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>.</td>\n",
       "      <td>_</td>\n",
       "      <td>yyDOT</td>\n",
       "      <td>yyDOT</td>\n",
       "      <td>_</td>\n",
       "      <td>19</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11279</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>עכשיו</td>\n",
       "      <td>עכשיו</td>\n",
       "      <td>RB</td>\n",
       "      <td>RB</td>\n",
       "      <td>_</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11280</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>קוראים</td>\n",
       "      <td>קרא</td>\n",
       "      <td>BN</td>\n",
       "      <td>BN</td>\n",
       "      <td>gen=M|num=P|per=A</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11281</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>את</td>\n",
       "      <td>את</td>\n",
       "      <td>AT</td>\n",
       "      <td>AT</td>\n",
       "      <td>_</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11282</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>ה</td>\n",
       "      <td>ה</td>\n",
       "      <td>DEF</td>\n",
       "      <td>DEF</td>\n",
       "      <td>_</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11283</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>קדיש</td>\n",
       "      <td>קדיש</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>gen=M|num=S</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11284</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>,</td>\n",
       "      <td>_</td>\n",
       "      <td>yyCM</td>\n",
       "      <td>yyCM</td>\n",
       "      <td>_</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11285</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>ו</td>\n",
       "      <td>ו</td>\n",
       "      <td>CONJ</td>\n",
       "      <td>CONJ</td>\n",
       "      <td>_</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11286</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>ה</td>\n",
       "      <td>ה</td>\n",
       "      <td>DEF</td>\n",
       "      <td>DEF</td>\n",
       "      <td>_</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11287</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>קהל</td>\n",
       "      <td>קהל</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>gen=M|num=S</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11288</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>עונה</td>\n",
       "      <td>ענה</td>\n",
       "      <td>BN</td>\n",
       "      <td>BN</td>\n",
       "      <td>gen=M|num=S|per=A</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11289</th>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>אמן</td>\n",
       "      <td>אמן</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>gen=M|num=S</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11290</th>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>.</td>\n",
       "      <td>_</td>\n",
       "      <td>yyDOT</td>\n",
       "      <td>yyDOT</td>\n",
       "      <td>_</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11291</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>תם</td>\n",
       "      <td>תם</td>\n",
       "      <td>VB</td>\n",
       "      <td>VB</td>\n",
       "      <td>gen=M|num=S|per=3|tense=PAST</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11292</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>מסע</td>\n",
       "      <td>מסע</td>\n",
       "      <td>NNT</td>\n",
       "      <td>NNT</td>\n",
       "      <td>gen=M|num=S</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11293</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>ה</td>\n",
       "      <td>ה</td>\n",
       "      <td>DEF</td>\n",
       "      <td>DEF</td>\n",
       "      <td>_</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11294</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>הלווייה</td>\n",
       "      <td>הלוויה</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>gen=F|num=S</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11295</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>.</td>\n",
       "      <td>_</td>\n",
       "      <td>yyDOT</td>\n",
       "      <td>yyDOT</td>\n",
       "      <td>_</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11296</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>מתחיל</td>\n",
       "      <td>התחיל</td>\n",
       "      <td>BN</td>\n",
       "      <td>BN</td>\n",
       "      <td>gen=M|num=S|per=A</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11297</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>מסע</td>\n",
       "      <td>מסע</td>\n",
       "      <td>NNT</td>\n",
       "      <td>NNT</td>\n",
       "      <td>gen=M|num=S</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11298</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>ה</td>\n",
       "      <td>ה</td>\n",
       "      <td>DEF</td>\n",
       "      <td>DEF</td>\n",
       "      <td>_</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11299</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>נקמה</td>\n",
       "      <td>נקמה</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>gen=F|num=S</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11300</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>.</td>\n",
       "      <td>_</td>\n",
       "      <td>yyDOT</td>\n",
       "      <td>yyDOT</td>\n",
       "      <td>_</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ZeroIx  Index     FORM   LEMMA    POS     PPOS  \\\n",
       "11271      17     18    גופתו    גופה     NN  NN_S_PP   \n",
       "11272      18     19       של      של    POS      POS   \n",
       "11273      19     20     מאיר    מאיר    NNP      NNP   \n",
       "11274      20     21     כהנא    כהנא    NNP      NNP   \n",
       "11275      21     22       אל      אל     IN       IN   \n",
       "11276      22     23        ה       ה    DEF      DEF   \n",
       "11277      23     24      קבר     קבר     NN       NN   \n",
       "11278      24     25        .       _  yyDOT    yyDOT   \n",
       "11279       0      1    עכשיו   עכשיו     RB       RB   \n",
       "11280       1      2   קוראים     קרא     BN       BN   \n",
       "11281       2      3       את      את     AT       AT   \n",
       "11282       3      4        ה       ה    DEF      DEF   \n",
       "11283       4      5     קדיש    קדיש     NN       NN   \n",
       "11284       5      6        ,       _   yyCM     yyCM   \n",
       "11285       6      7        ו       ו   CONJ     CONJ   \n",
       "11286       7      8        ה       ה    DEF      DEF   \n",
       "11287       8      9      קהל     קהל     NN       NN   \n",
       "11288       9     10     עונה     ענה     BN       BN   \n",
       "11289      10     11      אמן     אמן     NN       NN   \n",
       "11290      11     12        .       _  yyDOT    yyDOT   \n",
       "11291       0      1       תם      תם     VB       VB   \n",
       "11292       1      2      מסע     מסע    NNT      NNT   \n",
       "11293       2      3        ה       ה    DEF      DEF   \n",
       "11294       3      4  הלווייה  הלוויה     NN       NN   \n",
       "11295       4      5        .       _  yyDOT    yyDOT   \n",
       "11296       0      1    מתחיל   התחיל     BN       BN   \n",
       "11297       1      2      מסע     מסע    NNT      NNT   \n",
       "11298       2      3        ה       ה    DEF      DEF   \n",
       "11299       3      4     נקמה    נקמה     NN       NN   \n",
       "11300       4      5        .       _  yyDOT    yyDOT   \n",
       "\n",
       "                                            FEAT  CorrespondingTokenIx  \\\n",
       "11271  gen=F|num=S|suf_gen=M|suf_num=S|suf_per=3                    13   \n",
       "11272                                          _                    14   \n",
       "11273                                          _                    15   \n",
       "11274                                          _                    16   \n",
       "11275                                          _                    17   \n",
       "11276                                          _                    18   \n",
       "11277                                gen=M|num=S                    18   \n",
       "11278                                          _                    19   \n",
       "11279                                          _                     1   \n",
       "11280                          gen=M|num=P|per=A                     2   \n",
       "11281                                          _                     3   \n",
       "11282                                          _                     4   \n",
       "11283                                gen=M|num=S                     4   \n",
       "11284                                          _                     5   \n",
       "11285                                          _                     6   \n",
       "11286                                          _                     6   \n",
       "11287                                gen=M|num=S                     6   \n",
       "11288                          gen=M|num=S|per=A                     7   \n",
       "11289                                gen=M|num=S                     8   \n",
       "11290                                          _                     9   \n",
       "11291               gen=M|num=S|per=3|tense=PAST                     1   \n",
       "11292                                gen=M|num=S                     2   \n",
       "11293                                          _                     3   \n",
       "11294                                gen=F|num=S                     3   \n",
       "11295                                          _                     4   \n",
       "11296                          gen=M|num=S|per=A                     1   \n",
       "11297                                gen=M|num=S                     2   \n",
       "11298                                          _                     3   \n",
       "11299                                gen=F|num=S                     3   \n",
       "11300                                          _                     4   \n",
       "\n",
       "       sent_id  \n",
       "11271       -1  \n",
       "11272       -1  \n",
       "11273       -1  \n",
       "11274       -1  \n",
       "11275       -1  \n",
       "11276       -1  \n",
       "11277       -1  \n",
       "11278       -1  \n",
       "11279       -1  \n",
       "11280       -1  \n",
       "11281       -1  \n",
       "11282       -1  \n",
       "11283       -1  \n",
       "11284       -1  \n",
       "11285       -1  \n",
       "11286       -1  \n",
       "11287       -1  \n",
       "11288       -1  \n",
       "11289       -1  \n",
       "11290       -1  \n",
       "11291       -1  \n",
       "11292       -1  \n",
       "11293       -1  \n",
       "11294       -1  \n",
       "11295       -1  \n",
       "11296       -1  \n",
       "11297       -1  \n",
       "11298       -1  \n",
       "11299       -1  \n",
       "11300       -1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train_hebtb-gold.lattices', sep='\\t', quotechar='\"', quoting=csv.QUOTE_NONE, header=None)\n",
    "train_df.columns = ['ZeroIx','Index','FORM', 'LEMMA', 'POS', 'PPOS', 'FEAT', 'CorrespondingTokenIx']\n",
    "train_df['sent_id'] = -1\n",
    "\n",
    "dev_df = pd.read_csv('dev_hebtb-gold.lattices', sep='\\t', quotechar='\"', quoting=csv.QUOTE_NONE, header=None)\n",
    "dev_df.columns = ['ZeroIx','Index','FORM', 'LEMMA', 'POS', 'PPOS', 'FEAT', 'CorrespondingTokenIx']\n",
    "dev_df['sent_id'] = -1\n",
    "\n",
    "test_df = pd.read_csv('test_hebtb-gold.lattices', sep='\\t', quotechar='\"', quoting=csv.QUOTE_NONE, header=None)\n",
    "test_df.columns = ['ZeroIx','Index','FORM', 'LEMMA', 'POS', 'PPOS', 'FEAT', 'CorrespondingTokenIx']\n",
    "test_df['sent_id'] = -1\n",
    "\n",
    "dev_df.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_sents(gold_df):\n",
    "    sentence_number = 0\n",
    "    for index in gold_df.index:\n",
    "        if gold_df.at[index, 'ZeroIx'] == 0:\n",
    "            sentence_number += 1\n",
    "        gold_df.at[index, 'sent_id'] = sentence_number\n",
    "    \n",
    "    \n",
    "number_sents(train_df)\n",
    "number_sents(dev_df)\n",
    "number_sents(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dev_sents = dev_df.groupby(['sent_id'],  axis='columns')\n",
    "# dev_sents.tolist()\n",
    "# print(dev_sents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_df = dev_df.drop(dev_df.index[dev_df.sent_id == 296])\n",
    "# dev_df = dev_df.drop(dev_df.index[dev_df.sent_id == 226])\n",
    "# dev_df = dev_df.drop(dev_df.index[dev_df.sent_id == 57])\n",
    "# dev_df = dev_df.drop(dev_df.index[dev_df.sent_id == 49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_df[dev_df['sent_id'] == 49]\n",
    "# len(dev_df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Getting sentences to give to bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490\n"
     ]
    }
   ],
   "source": [
    "class sentenceGetter(object):\n",
    "    def __init__(self, dataframe, max_sent=None):\n",
    "        self.index = 0\n",
    "        self.max_sent = max_sent\n",
    "        self.tokens = dataframe['FORM']\n",
    "        self.labels = dataframe['POS']\n",
    "        #for evaluating by word-accuracy\n",
    "        self.correspondingToken = dataframe['CorrespondingTokenIx']\n",
    "        self.orig_sent_id = dataframe['sent_id']\n",
    "    \n",
    "    def sentences(self):\n",
    "        sent = []\n",
    "        counter = 0\n",
    "        \n",
    "        for token,label, corres_tok, sent_id in zip(self.tokens, self.labels, self.correspondingToken, self.orig_sent_id):\n",
    "            sent.append((token, label, corres_tok, sent_id))\n",
    "            if token.strip() == \".\":\n",
    "                yield sent\n",
    "                sent = []\n",
    "                counter += 1\n",
    "            if self.max_sent is not None and counter >= self.max_sent:\n",
    "                return\n",
    "\n",
    "train_getter = sentenceGetter(train_df)\n",
    "dev_getter = sentenceGetter(dev_df)\n",
    "test_getter = sentenceGetter(test_df)\n",
    "\n",
    "train_sentences = [[token for token, label, corres_tok, sent_id in sent] for sent in train_getter.sentences()]\n",
    "train_labels = [[label for token, label, corres_tok, sent_id in sent] for sent in train_getter.sentences()]\n",
    "\n",
    "dev_sentences = [[token for token, label, corres_tok, sent_id in sent] for sent in dev_getter.sentences()]\n",
    "dev_labels = [[label for token, label, corres_tok, sent_id in sent] for sent in dev_getter.sentences()]\n",
    "dev_corresTokens = [[corres_tok for token, label, corres_tok, sent_id in sent] for sent in dev_getter.sentences()]\n",
    "dev_sent_ids = [[sent_id for token, label, corres_tok, sent_id in sent] for sent in dev_getter.sentences()]\n",
    "\n",
    "test_sentences = [[token for token, label, corres_tok, sent_id in sent] for sent in test_getter.sentences()]\n",
    "test_labels = [[label for token, label, corres_tok, sent_id in sent] for sent in test_getter.sentences()]\n",
    "\n",
    "print(len(dev_sentences))\n",
    "# print(train_sentences[0])\n",
    "# print(train_labels[0])\n",
    "# print(type(train_sentences))\n",
    "dummy_sentences = dev_sentences[0:2]\n",
    "dummy_labels = dev_labels[0:2]\n",
    "# print(dev_sentences[49])\n",
    "# print(dummy_sentences[49])\n",
    "# print(dummy_labels[49])\n",
    "# sent = dummy_sentences[35]\n",
    "# labs = dummy_labels[35]\n",
    "# for word, lab in zip(sent, labs):\n",
    "#     print(word, lab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# longest_sent_len = 0\n",
    "# for sent in dev_sentences:\n",
    "#     if len(sent) >= longest_sent_len:\n",
    "#         print(len(sent))\n",
    "#         longest_sent_len = len(sent)\n",
    "#         print(\"index of longest sentence:{} \".format(dev_sentences.index(sent)))\n",
    "\n",
    "# longest_sent_len\n",
    "\n",
    "del dev_sentences[296]\n",
    "del dev_labels[296]\n",
    "del dev_corresTokens[296]\n",
    "del dev_sent_ids[296]\n",
    "\n",
    "\n",
    "del dev_sentences[226]\n",
    "del dev_labels[226]\n",
    "del dev_corresTokens[226]\n",
    "del dev_sent_ids[226]\n",
    "\n",
    "\n",
    "del dev_sentences[57]\n",
    "del dev_labels[57]\n",
    "del dev_corresTokens[57]\n",
    "del dev_sent_ids[57]\n",
    "\n",
    "\n",
    "del dev_sentences[49]\n",
    "del dev_labels[49]\n",
    "del dev_corresTokens[49]\n",
    "del dev_sent_ids[49]\n",
    "\n",
    "\n",
    "\n",
    "# for sent in test_sentences:\n",
    "#     if len(sent) >= longest_sent_len:\n",
    "#         print(len(sent))\n",
    "#         longest_sent_len = len(sent)\n",
    "#         print(\"index of longest sentence:{} \".format(test_sentences.index(sent)))\n",
    "\n",
    "# longest_sent_len\n",
    "# del test_sentences[396]\n",
    "# del test_labels[396]\n",
    "# del test_sentences[394]\n",
    "# del test_labels[394]\n",
    "# del test_sentences[387]\n",
    "# del test_labels[387]\n",
    "# del test_sentences[151]\n",
    "# del test_labels[151]\n",
    "# del test_sentences[134]\n",
    "# del test_labels[134]\n",
    "# del test_sentences[104]\n",
    "# del test_labels[104]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Number of gpus: 4\n",
      "Name of gpu: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "print(\"Device: \" + str(device))\n",
    "print(\"Number of gpus: \" + str(n_gpu))\n",
    "print(\"Name of gpu: \" + torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PAD', 'yyCM', 'yyDOT', ' PREPOSITION', 'QW', 'CDT', 'yyLRB', 'JJ', 'PREPOSITION', 'yyQM', 'REL', 'ADVERB', '??', 'PREPOSITIONIN', 'yyCLN', 'DTT', 'BN', 'TEMP', 'NNT', 'POS', 'NN', 'NNP', 'CONJ', 'AT', 'CD', 'EX', 'PRP', 'JJT', 'yySCLN', 'COP', 'S_ANP', 'ZVL', 'NCD', 'CC', 'yyEXCL', 'S_PRN', 'DUMMY_AT', 'INTJ', 'IN', 'P', 'BNT', 'DT', 'VB', 'MD', 'RB', 'NEG', 'yyQUOT', 'yyELPS', 'yyDASH', 'DEF', 'yyRRB']\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "tags_data = pd.concat([train_df, dev_df, test_df])\n",
    "tag_vals = list(set(tags_data[\"POS\"].values))\n",
    "tags = ['PAD'] + tag_vals\n",
    "print(tags)\n",
    "print(len(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2idx = {tag:idx for idx, tag in enumerate(tags)}\n",
    "idx2tag = {idx:tag for idx, tag in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('./multi_cased_L-12_H-768_A-12/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_sentences[0])\n",
    "# print(train_labels[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Constants for further usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 150\n",
    "bs = 32"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Tokenizing sentences for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['הם', 'התבקשו', 'לדווח', 'ל', 'ה', 'משטרה', 'על', 'תנועותיהם', '.']\n",
      "['הם', 'ה', '##ת', '##בק', '##שו', 'ל', '##דו', '##וח', 'ל', 'ה', 'מ', '##ש', '##טר', '##ה', 'על', 'ת', '##נוע', '##ות', '##יהם', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./multi_cased_L-12_H-768_A-12/')\n",
    "\n",
    "def tokenize(sentences, orig_labels):    \n",
    "    tokenized_texts = []\n",
    "    labels = []\n",
    "    sents, tags_li = [], []\n",
    "    for sent, sent_labels in zip(sentences, orig_labels):\n",
    "        bert_tokens = []\n",
    "        bert_labels = []\n",
    "        for orig_token, orig_label in zip(sent, sent_labels):\n",
    "            b_tokens = tokenizer.tokenize(orig_token)\n",
    "            bert_tokens.extend(b_tokens)\n",
    "            for b_token in b_tokens:\n",
    "                bert_labels.append(orig_label)\n",
    "        tokenized_texts.append(bert_tokens)\n",
    "        labels.append(bert_labels)\n",
    "        assert len(bert_tokens) == len(bert_labels)\n",
    "    return tokenized_texts, labels\n",
    "\n",
    "train_tokenized_texts, train_tokenized_labels = tokenize(train_sentences, train_labels)\n",
    "print(train_sentences[10])\n",
    "print(train_tokenized_texts[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def pad_sentences_and_labels(tokenized_texts, labels):\n",
    "    input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                              maxlen = MAX_LEN, dtype = \"int\", truncating = \"post\", padding = \"post\")\n",
    "    tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels], \n",
    "                         maxlen = MAX_LEN, value = tag2idx['PAD'], padding = \"post\",\n",
    "                        dtype = \"int\", truncating = \"post\")\n",
    "    attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n",
    "    return input_ids, tags, attention_masks\n",
    "\n",
    "input_ids, tags, attention_masks = pad_sentences_and_labels(train_tokenized_texts, train_tokenized_labels)\n",
    "# print(tags)\n",
    "# print(input_ids)\n",
    "# print(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(input_ids, dtype=torch.long)\n",
    "tr_tags = torch.tensor(tags, dtype=torch.long)\n",
    "tr_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
    "\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "# train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.tokenize(\"הוא הגיש תלונה במשטרה\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   7%|▋         | 1/15 [01:21<19:04, 81.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.8764334999417004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  13%|█▎        | 2/15 [02:45<17:52, 82.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.23451287828777967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|██        | 3/15 [04:10<16:36, 83.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.14928182155678146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  27%|██▋       | 4/15 [05:37<15:26, 84.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10477634311016452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  33%|███▎      | 5/15 [07:09<14:27, 86.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07998400458477829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|████      | 6/15 [08:34<12:54, 86.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.06014351236731991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  47%|████▋     | 7/15 [09:59<11:25, 85.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.04574003522774499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  53%|█████▎    | 8/15 [11:23<09:57, 85.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.039042434220104234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|██████    | 9/15 [12:48<08:30, 85.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.030989381172204095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  67%|██████▋   | 10/15 [14:12<07:04, 84.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.02690853578602209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  73%|███████▎  | 11/15 [15:36<05:38, 84.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.02274791951487331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|████████  | 12/15 [17:00<04:13, 84.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.019322652587250463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  87%|████████▋ | 13/15 [18:24<02:48, 84.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.017739925548850902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  93%|█████████▎| 14/15 [19:49<01:24, 84.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.01617034224297018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 100%|██████████| 15/15 [21:13<00:00, 84.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.012969936423408063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained('./multi_cased_L-12_H-768_A-12', num_labels=len(tag2idx))\n",
    "model.cuda()\n",
    "\n",
    "# lr = 1e-3\n",
    "# max_grad_norm = 1.0\n",
    "# num_total_steps = 1000\n",
    "# num_warmup_steps = 100\n",
    "# warmup_proportion = float(num_warmup_steps) / float(num_total_steps)\n",
    "\n",
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n",
    "\n",
    "\n",
    "# optimizer = AdamW(model.parameters(), lr=lr, correct_bias=False)\n",
    "# scheduler = WarmupLinearSchedule(optimizer, warmup_steps=num_warmup_steps, t_total=num_total_steps)\n",
    "\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "epochs = 15\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # TRAIN loop\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "#         print(\"Input: \")\n",
    "#         print(b_input_ids)\n",
    "#         print(\"Labels: \")\n",
    "#         print(b_labels)\n",
    "        # forward pass\n",
    "        loss = model(b_input_ids, token_type_ids=None,\n",
    "                     attention_mask=b_input_mask, labels=b_labels)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss / nb_tr_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = \"./\"\n",
    "# model_to_save = model.module if hasattr(model, 'module') else model\n",
    "\n",
    "# output_model_file = os.path.join(output_dir, 'pytorch_model.bin')\n",
    "# output_config_file = os.path.join(output_dir, 'bert_config.json')\n",
    "\n",
    "# torch.save(model_to_save.state_dict(), output_model_file)\n",
    "# model_to_save.config.to_json_file(output_config_file)\n",
    "# tokenizer.save_vocabulary(output_dir)\n",
    "\n",
    "\n",
    "#     model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "#     output_model_file = args.output_dir / \"pytorch_model.bin\"\n",
    "#     torch.save(model_to_save.state_dict(), str(output_model_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertForTokenClassification.from_pretrained(output_dir)\n",
    "# tokenizer = BertTokenizer.from_pretrained(output_dir, do_lower_case=args.do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function receives a sentence with its labels, and the tokenized sentence and labels\n",
    "def aggr_toks_labels_tags(orig_words, orig_labels, tok_wordps, tok_labels, predicted_tags, test_tags):\n",
    "    \n",
    "    joint_tokens = []\n",
    "    joint_labels = []\n",
    "    joint_predicted = []\n",
    "    joint_test = []\n",
    "    \n",
    "    for word in orig_words:\n",
    "        aggregated_tokenized = \"\"\n",
    "        aggregated_label = \"\"\n",
    "        aggregated_predicted = \"\"\n",
    "        aggregated_test = \"\"\n",
    "        \n",
    "        while aggregated_tokenized != word:\n",
    "#             print(len(tok_sent))\n",
    "            tmpTok = tok_wordps.pop(0)\n",
    "#             print(tmpTok)\n",
    "#             print(joint_tokens)\n",
    "            if tmpTok.startswith(\"##\"):\n",
    "                tmpTok = tmpTok[2:]\n",
    "                \n",
    "            tmpLab = tok_labels.pop(0)\n",
    "            if aggregated_label == \"\":\n",
    "                aggregated_label = tmpLab\n",
    "                \n",
    "            tmpPred = predicted_tags.pop(0)\n",
    "            if aggregated_predicted == \"\":\n",
    "                aggregated_predicted = tmpPred\n",
    "                \n",
    "            tmpTest = test_tags.pop(0)\n",
    "            if aggregated_test == \"\":\n",
    "                aggregated_test = tmpTest\n",
    "                \n",
    "            aggregated_tokenized += tmpTok\n",
    "#             print(aggregated_tokenized)\n",
    "            \n",
    "        joint_tokens.append(aggregated_tokenized)\n",
    "        joint_labels.append(aggregated_label)\n",
    "        joint_predicted.append(aggregated_predicted)\n",
    "        joint_test.append(aggregated_test)\n",
    "        \n",
    "    assert len(joint_tokens) == len(orig_words)\n",
    "    assert len(joint_tokens) == len(joint_predicted)\n",
    "    return joint_tokens, joint_labels, joint_predicted, joint_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stav/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/stav/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (F1): = 0.9556933175312243\n"
     ]
    }
   ],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def delete_pads_from_preds(predicted_tags, test_tags):\n",
    "    clean_predicted = []\n",
    "    clean_test = []\n",
    "    \n",
    "    for ix in range(0, len(test_tags)):\n",
    "        if test_tags[ix] != 'PAD':\n",
    "            clean_predicted.append(predicted_tags[ix])\n",
    "            clean_test.append(test_tags[ix])\n",
    "            \n",
    "    return clean_predicted, clean_test\n",
    "    \n",
    "def calculate_accuracy(df):\n",
    "    numOfCorrectPredictions = 0\n",
    "    for index in df.index:\n",
    "        orig_pos = df.at[index, 'test_tag']\n",
    "        pred_pos = df.at[index, 'predicted_tag']\n",
    "        if orig_pos == pred_pos:\n",
    "            numOfCorrectPredictions += 1\n",
    "    return numOfCorrectPredictions/len(df)\n",
    "                \n",
    "def test_model(sentence, labels, tok_sent, tok_labels, corres_tokens, sent_id):\n",
    "    input_ids, tags, attention_masks = pad_sentences_and_labels([tok_sent], [tok_labels])\n",
    "\n",
    "    val_inputs = torch.tensor(input_ids, dtype=torch.long)\n",
    "    val_tags = torch.tensor(tags, dtype=torch.long)\n",
    "    val_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
    "\n",
    "    test_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=bs)\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions, true_labels = [], []\n",
    "    counter = 0\n",
    "    for batch in test_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "                                attention_mask=b_input_mask, labels=b_labels)\n",
    "            logits = model(b_input_ids, token_type_ids=None,\n",
    "                         attention_mask=b_input_mask)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        predictions.append([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        \n",
    "        true_labels.append(label_ids)\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    \n",
    "    pred_tags = [idx2tag[p_ii] for p in predictions for p_i in p for p_ii in p_i]\n",
    "    test_tags = [idx2tag[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
    "#     print(list(zip(pred_tags, test_tags)))\n",
    "    # -----------------------------------------------------------------------\n",
    "    clean_predicted, clean_test = delete_pads_from_preds(pred_tags, test_tags)\n",
    "    joint_tokenized, joint_labels, preds, tests = aggr_toks_labels_tags(sentence, labels, tok_sent, tok_labels, \n",
    "                                                                        clean_predicted, clean_test)\n",
    "    \n",
    "    tmp = {'word': sentence, 'orig_label': labels, 'predicted_tag': preds, 'test_tag': tests, \n",
    "           'corresToken': corres_tokens, 'sent_id': sent_id}\n",
    "    tmp_df = pd.DataFrame(data=tmp)\n",
    "    # -----------------------------------------------------------------------\n",
    "    \n",
    "    y_true = pd.Series(test_tags)\n",
    "    y_pred = pd.Series(pred_tags)\n",
    "    cross_tab = pd.crosstab(y_true, y_pred, rownames=['Real Label'], colnames=['Prediction'], margins=True)\n",
    "    report = classification_report(y_true, y_pred)\n",
    "#     print(report)\n",
    "#     print(tmp_df)\n",
    "    return tmp_df\n",
    "\n",
    "full_df = pd.DataFrame()\n",
    "dev_tokenized_texts, dev_tokenized_labels = tokenize(dev_sentences, dev_labels)\n",
    "for sent, label, tok_sent, tok_label, corresTokens, sent_id in zip(dev_sentences, dev_labels, dev_tokenized_texts, \n",
    "                                                                   dev_tokenized_labels, dev_corresTokens, dev_sent_ids):\n",
    "    test_df = test_model(sent, label, tok_sent, tok_label, corresTokens, sent_id)\n",
    "    full_df = full_df.append(test_df, ignore_index=True, sort=False)\n",
    "\n",
    "# full_df\n",
    "f1_accuracy = calculate_accuracy(full_df)\n",
    "print(\"Accuracy (F1): = {}\".format(f1_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_csv('BE-Oracle-tag-5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_acc_df = full_df.groupby(['sent_id', 'corresToken']).apply(lambda x: '^'.join(x.predicted_tag)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_acc_df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_acc_df.to_csv('word-acc-Oracle-tag-5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
